














import numpy as np 
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, precision_recall_curve, auc
from plotly.subplots import make_subplots
import itertools
# Run the following two lines of code for Uncaught Error: Script error for plotly
from plotly.offline import plot, iplot, init_notebook_mode
init_notebook_mode(connected=True)











df = pd.read_csv('BCD_data.csv')
df.head()





missing_values_count = df.isnull().sum()
missing_values_count





df.drop(['id','Unnamed: 32'],axis=1,inplace=True)











df.shape





df.describe()








names = df.columns[1:10]
values=[] 
for column in df.iloc[:,1:10].columns:
    li = df[column].tolist()
    values.append(li)
colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen','cyan', 'royalblue']

fig = go.Figure()
for xd, yd, cls in zip(names, values, colors):
        fig.add_trace(go.Box(
            y=yd,
            name=xd,
            boxpoints='outliers',
            jitter=0.5,
            whiskerwidth=0.2,
            fillcolor=cls,
            marker_size=3,
            line_width=2)
        )
fig.update_layout(title="箱线图1 - 程慧", title_x=0.5)
fig.show()


names = df.columns[11:20]
values=[] 
for column in df.iloc[:,11:20].columns:
    li = df[column].tolist()
    values.append(li)
colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen','cyan', 'royalblue']

fig = go.Figure()
for xd, yd, cls in zip(names, values, colors):
        fig.add_trace(go.Box(
            y=yd,
            name=xd,
            boxpoints='outliers',
            jitter=0.5,
            whiskerwidth=0.2,
            fillcolor=cls,
            marker_size=3,
            line_width=2)
        )
fig.update_layout(title="箱线图2 - 程慧", title_x=0.5)
fig.show()


names = df.columns[21:30]
values=[] 
for column in df.iloc[:,21:30].columns:
    li = df[column].tolist()
    values.append(li)
colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen','cyan', 'royalblue']

fig = go.Figure()
for xd, yd, cls in zip(names, values, colors):
        fig.add_trace(go.Box(
            y=yd,
            name=xd,
            boxpoints='outliers',
            jitter=0.5,
            whiskerwidth=0.2,
            fillcolor=cls,
            marker_size=3,
            line_width=2)
        )
fig.update_layout(title="箱线图3 - 程慧", title_x=0.5)
fig.show()











fig = go.Figure(data=[go.Pie(labels=['Benign','Malignant'], values=df['diagnosis'].value_counts(), textinfo='label+percent')])
fig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,
                  marker=dict(colors=['gold', 'mediumturquoise'], line=dict(color='#000000', width=2)))
fig.update_layout(title={'text': "乳腺癌数据分布 - 程慧", 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'})
fig.show()











# 绘制成对分析图
sns.pairplot(df.iloc[:, :11], hue='diagnosis', diag_kind='hist', height=1.6)
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False 
plt.suptitle('成对分析图 - 程慧', y=1.02, fontsize=16, ha='center')











corr = df.iloc[:,1:].corr()
fig = go.Figure(data=go.Heatmap(z=np.array(corr),x=corr.columns.tolist(),y=corr.columns.tolist(),xgap = 1,ygap = 1))
fig.update_layout(
    title={'text': "特征相关性热力图 - 程慧", 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'},
    margin=dict(t=25, r=0, b=200, l=200),
    width=1000,
    height=700
)
fig.show()











df.diagnosis.unique()








from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis']) # M:1, B:0
df['diagnosis'].value_counts()








random_state = 42
X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df['diagnosis'], test_size = 0.2, random_state = random_state)





scale = RobustScaler()
X_train = scale.fit_transform(X_train)
X_test = scale.transform(X_test)














pca = PCA()
pca.fit(X_train)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)

fig = px.line(x=np.arange(1,exp_var_cumul.shape[0]+1), y=exp_var_cumul, markers=True, labels={'x':'主成分数量', 'y':'累计解释方差'})

#fig.add_shape(type='line', line=dict(dash='dash'),x0=0, x1=30, y0=0.95, y1=0.95)#标记可解释方差阈值
fig.update_layout(title={'text': "PCA中累计解释方差与主成分数量关系 - 程慧", 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'},
                  xaxis_title='主成分数量',
                  yaxis_title='累计解释方差',
                  width=800, height=500)
fig.show()











rfecv = RFECV(cv=StratifiedKFold(n_splits=10, random_state=random_state, shuffle=True),
      estimator=DecisionTreeClassifier(), scoring='accuracy')#十折交叉验证
    
rfecv.fit(X_train, y_train)

print("最优特征数: %d" % rfecv.n_features_)#打印最优特征数量

plt.figure(figsize=(15,5))
plt.xlabel("# of features selected")
plt.ylabel("Cross validation score (accuracy)")
plt.plot(
    range(1, len(rfecv.cv_results_['mean_test_score']) + 1),
    rfecv.cv_results_['mean_test_score'],
)
plt.title("RFECV - 程慧")
plt.show()


selected_features = df.columns[1:][rfecv.get_support()]
print("Selected optimal features:", selected_features.tolist())











def modelselection(classifier, parameters, scoring, X_train):
    clf = GridSearchCV(estimator=classifier,
                   param_grid=parameters,
                   scoring= scoring,
                   cv=5,
                   n_jobs=-1)
    clf.fit(X_train, y_train)
    cv_results = clf.cv_results_
    best_parameters = clf.best_params_
    best_result = clf.best_score_
    print('The best parameters for classifier is', best_parameters)
    print('The best training score is %.3f:'% best_result)
    return cv_results, best_parameters, best_result














def PCA_curves(PCA_cv_score, PCA_test_score, training_time):
    # PCA_cv_score: 交叉验证分数
    # PCA_test_score: 测试分数
    # training_time: 训练时间
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter"}, {"type": "scatter"}]],
    subplot_titles=('# of Components in PCA versus Model Accuracy','# of Components in PCA versus Training Time')
    )
    
    fig.add_trace(go.Scatter(x=n,y=PCA_cv_score,
                             line=dict(color='rgb(231,107,243)', width=2), name='CV score'),
                  row=1, col=1)
    fig.add_trace(go.Scatter(x=n,y=PCA_test_score,
                             line=dict(color='rgb(0,176,246)', width=2), name='Test score'),              
                  row=1, col=1)    
    fig.add_trace(go.Scatter(x=n,y=training_time,
                             line=dict(color='rgb(0,100,80)', width=2), name='Training time'),
                  row=1, col=2)
    fig.update_xaxes(title_text='# of components')
    fig.update_layout(title={'text': "PCA主成分数量与模型准确率/训练时间关系图 - 程慧", 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'})
    fig.update_yaxes(title_text='Accuracy', row=1, col=1)
    # fig.update_xaxes(title_text="Recall", row=1, col=2)
    fig.update_yaxes(title_text='Training time', row=1, col=2)
    fig.show()








def metrics(X,CV_clf):
    # X: test data
    # CV_clf: trained classifier
    y_pred = CV_clf.predict(X)
    cm = confusion_matrix(y_test, y_pred)
    tn = cm[0,0]
    fp = cm[0,1]
    fn = cm[1,0]
    tp = cm[1,1]
    Accuracy=(tp+tn)/(tp+tn+fp+fn)
    Sensitivity=tp/(tp+fn)
    Specificity=tn/(tn+fp)
    Precision=tp/(tp+fp)
    F_measure=2*tp/(2*tp+fp+fn)
    print('Accuracy=%.3f'%Accuracy)
    print('Sensitivity=%.3f'%Sensitivity) # as the same as recall
    print('Specificity=%.3f'%Specificity)
    print('Precision=%.3f'%Precision)
    print('F-measure=%.3f'%F_measure)
    return Accuracy, Sensitivity, Specificity, Precision, F_measure
 #   plot_confusion_matrix(CV_clf, X_test, y_test)








def plot_roc_curve(y_test, y_score):
    # y_test: 测试集的真实标签
    # y_score: 模型预测的概率分数
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)  # 计算AUC值

    # 绘制ROC曲线
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random guess')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.show()








def plot_roc_prc(y_test, y_score):
    # 计算 ROC 曲线和 AUC
    fpr, tpr, roc_thresholds = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    precision, recall, prc_thresholds = precision_recall_curve(y_test, y_score)
    prc_auc = auc(recall, precision)
    fig = make_subplots(
        rows=1, cols=2,
        specs=[[{"type": "scatter"}, {"type": "scatter"}]],
        subplot_titles=(f'ROC Curve (AUC={roc_auc:.4f})', f'Precision-Recall Curve (AUC={prc_auc:.4f})')
    )

    # 绘制 ROC 曲线
    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve'), row=1, col=1)
    fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1, row=1, col=1)

    # 绘制 Precision-Recall 曲线
    fig.add_trace(go.Scatter(x=recall, y=precision, mode='lines', name='Precision-Recall Curve'), row=1, col=2)
    fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0.5, y1=0.5, row=1, col=2)

    fig.update_xaxes(title_text="False Positive Rate / 1-Specificity", row=1, col=1)
    fig.update_yaxes(title_text="True Positive Rate / Recall", row=1, col=1)
    fig.update_xaxes(title_text="Recall", row=1, col=2)
    fig.update_yaxes(title_text="Precision", row=1, col=2)

    fig.show()
    return roc_thresholds, prc_thresholds














from sklearn.ensemble import GradientBoostingClassifier

classifier_gb = GradientBoostingClassifier(random_state=random_state)
parameters_gb = {#定义超参数搜索范围
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}
scoring_gb = 'accuracy'





gb_clf = GradientBoostingClassifier(n_estimators=100,
                                     learning_rate=0.1,
                                     max_depth=3,
                                     random_state=random_state)
gb_clf.fit(X_train, y_train)

# Metrics
gb_metrics = metrics(X_test, gb_clf)








def compare_pca_gb(n_components):
    cv_score, test_score, cv_training_time = [], [], []
    for n in n_components:
        print("The number of components in PCA is:%d " % n)
        
        pca = PCA(n_components=n, svd_solver="full", random_state=random_state)
        X_train_pca = pca.fit_transform(X_train)
        X_test_pca = pca.transform(X_test)
        
        # Model Selection
        cv_results, best_param, best_result = modelselection(classifier_gb, parameters_gb, scoring_gb, X_train_pca)
        training_time = np.mean(np.array(cv_results['mean_fit_time']) + np.array(cv_results['mean_score_time']))
        cv_score.append(best_result)
        cv_training_time.append(training_time)
        
        # Train the model with the best parameters
        CV_clf = GradientBoostingClassifier(n_estimators=best_param['n_estimators'],
                                             learning_rate=best_param['learning_rate'],
                                             max_depth=best_param['max_depth'],
                                             random_state=random_state)
        CV_clf.fit(X_train_pca, y_train)
        score = CV_clf.score(X_test_pca, y_test)
        test_score.append(score)
    
    return cv_score, test_score, cv_training_time


n_features = X_train.shape[1]
n = np.arange(2, n_features+2, 2) 

PCA_cv_score, PCA_test_score, PCA_cv_training_time= compare_pca_gb(n_components = n)


# 打印最好的准确率及相应的特征数量
print('The best accuracy of is: %.3f' % max(PCA_test_score) + ', where the total number of features selected is %d' % rfecv.n_features_)

# 获取最佳准确率的索引
best_index = np.argmax(PCA_test_score)

best_model = GradientBoostingClassifier(n_estimators=parameters_gb['n_estimators'][best_index // 9],
										learning_rate=parameters_gb['learning_rate'][(best_index % 9) // 3],
										max_depth=parameters_gb['max_depth'][best_index % 3],
										random_state=random_state)
best_model.fit(X_train, y_train)

# 打印性能指标
print(metrics(X_test, best_model))





PCA_curves(PCA_cv_score,PCA_test_score,PCA_cv_training_time)








pca = PCA(n_components=(i+1)*2, svd_solver="full", random_state=random_state)
X_PCA_train = pca.fit_transform(X_train)
X_PCA_test = pca.transform(X_test)
# Model Selection
cv_results, best_param, best_result = modelselection(classifier_gb, parameters_gb, scoring_gb, X_PCA_train)

# Classifier with the best hyperparameters
gb_PCA = GradientBoostingClassifier(n_estimators=best_param['n_estimators'],
                                    learning_rate=best_param['learning_rate'],
                                    max_depth=best_param['max_depth'],
                                    random_state=random_state)
gb_PCA.fit(X_PCA_train, y_train)

# Metrics
gb_PCA_metrics = metrics(X_PCA_test, gb_PCA)






thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))

for n, ax in zip(thresholds, axs.ravel()):
    y_score = gb_PCA.predict_proba(X_PCA_test)[:, 1] > n
    
    cm = confusion_matrix(y_test, y_score)
    
    tp = cm[1, 1]
    fn = cm[1, 0]
    fp = cm[0, 1]
    tn = cm[0, 0]

    print('threshold = %s :' % n,
          'Accuracy={:.3f}'.format((tp + tn) / (tp + tn + fp + fn)),
          'Sensitivity={:.3f}'.format(tp / (tp + fn)),
          'Specificity={:.3f}'.format(tn / (tn + fp)),
          'Precision={:.3f}'.format(tp / (tp + fp)))
    
    im = ax.matshow(cm, cmap='Blues', alpha=0.7)

    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(j, i, cm[i, j], horizontalalignment='center')
        
    ax.set_ylabel('True label', fontsize=12)
    ax.set_xlabel('Predicted label', fontsize=12)
    ax.set_title('Threshold = %s' % n, fontsize=12)
    fig.colorbar(im, ax=ax, orientation='vertical')
    
    fig.suptitle('不同阈值下的模型性能指标 - 程慧', fontsize=16, y=0.92)
plt.show()

















parameters_rf = {
    'n_estimators': [20, 50, 100, 150, 200],
    'criterion': ['gini', 'entropy'],
    'bootstrap': [True, False],
}

scoring_rf = 'accuracy' 
classifier_rf = RandomForestClassifier(random_state=random_state)

#超参数搜索
cv_results, best_param, best_result = modelselection(classifier_rf,parameters_rf, scoring_rf, X_train)

# Classifier with the best hyperparameters
rf_clf = RandomForestClassifier(n_estimators = best_param['n_estimators'],
                                criterion = best_param['criterion'],
                                bootstrap = best_param['bootstrap'],
                                random_state=random_state)
rf_clf.fit(X_train, y_train)

rf_metrics = metrics(X_test,rf_clf)











def compare_pca(n_components):
    cv_score, test_score, cv_training_time = [], [], []
    for n in n_components:
        print("The number of components in PCA is:%d "% n)
        pca = PCA(n_components=n, svd_solver="full",random_state=random_state)
        X_PCA_train = pca.fit_transform(X_train)
        X_PCA_test = pca.transform(X_test)
        # Model Selection
        cv_results, best_param, best_result = modelselection(classifier_rf, parameters_rf, scoring_rf, X_PCA_train)
        training_time = np.mean(np.array(cv_results['mean_fit_time'])+np.array(cv_results['mean_score_time']))
        cv_score.append(best_result)
        cv_training_time.append(training_time)
        CV_clf = RandomForestClassifier(n_estimators = best_param['n_estimators'],
                                        criterion = best_param['criterion'],
                                        bootstrap = best_param['bootstrap'],
                                        random_state=random_state)
        CV_clf.fit(X_PCA_train, y_train)
        score = CV_clf.score(X_PCA_test, y_test)
        test_score.append(score)
    print(cv_score, test_score, cv_training_time)
    return cv_score, test_score, cv_training_time


n_features = X_train.shape[1]
n = np.arange(2, n_features+2, 2) 
PCA_cv_score, PCA_test_score, PCA_cv_training_time = compare_pca(n_components=n)


# 打印最好的准确率及相应的特征数量
print(f"The best accuracy is: {max(PCA_test_score):.3f}, where the total number of features selected is {rfecv.n_features_}")

best_index = np.argmax(PCA_test_score)
best_model = RandomForestClassifier(n_estimators=parameters_rf['n_estimators'][best_index // 6],
                                     criterion=parameters_rf['criterion'][(best_index % 6) // 3],
                                     bootstrap=parameters_rf['bootstrap'][best_index % 3],
                                     random_state=random_state)
best_model.fit(X_train, y_train)
# 打印性能指标
print(metrics(X_test, best_model))






PCA_curves(PCA_cv_score, PCA_test_score, PCA_cv_training_time)











X_train_selected = X_train[:, rfecv.get_support()]
X_test_selected = X_test[:, rfecv.get_support()]

cv_results, best_param, best_result = modelselection(classifier_gb, parameters_gb, scoring_gb, X_train_selected)

# Classifier with the best hyperparameters
gb_RFE = GradientBoostingClassifier(n_estimators=best_param['n_estimators'],
                                    learning_rate=best_param['learning_rate'],
                                    max_depth=best_param['max_depth'],
                                    random_state=random_state)
gb_RFE.fit(X_train_selected, y_train)

# Metrics
gb_RFE_metrics = metrics(X_test_selected, gb_RFE)









X_train_selected = X_train[:,rfecv.get_support()]
X_test_selected = X_test[:,rfecv.get_support()]

cv_results, best_param, best_result = modelselection(classifier_rf,parameters_rf, scoring_rf, X_train_selected)

# Classifier with the best hyperparameters
rf_RFE = RandomForestClassifier(n_estimators = best_param['n_estimators'],
                                criterion = best_param['criterion'],
                                bootstrap = best_param['bootstrap'],
                                random_state=random_state)
rf_RFE.fit(X_train_selected, y_train)

# Metrics
rf_RFE_metrics = metrics(X_test_selected ,rf_RFE)








# 模型名称
models = ['RandomForest', 'Rand+PCA', 'Rand+RFE', 'GradientBoosting', 'GB+PCA', 'GB+RFE']

# 模型性能指标
metrics_to_plot = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision']
colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen', 'cyan', 'royalblue']

# 定义 df_scores 数据框
df_scores = pd.DataFrame({
    'GradientBoosting': [0.956, 0.930, 0.972, 0.952],
    'GB+PCA': [0.965, 0.930, 0.986, 0.976],
    'GB+RFE': [0.965, 0.930, 0.986, 0.976],
    'RandomForest': [0.956, 0.930, 0.972, 0.952],
    'Rand+PCA': [0.974, 0.930, 0.986, 0.976],
    'Rand+RFE': [0.967, 0.930, 0.972, 0.952]
}, index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision'])

# 提取数据
data = df_scores.loc[metrics_to_plot].T

# 创建子图
fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(20, 5), sharey=True)

# 绘制每个性能指标的柱状图
for ax, metric in zip(axes, metrics_to_plot):
    ax.bar(models, data[metric], color=colors)
    ax.set_title(f'{metric} Comparison')
    ax.set_xlabel('Models')
    ax.set_ylabel('Score')
    ax.set_ylim(0.9, 1.0)  # 设置y轴范围
    ax.tick_params(axis='x', rotation=45)

# 调整布局
plt.tight_layout()
plt.show()






